{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Understanding Activation Functions</h1></center>\n",
    "\n",
    "**Goal:** Understand the different activation functions and their common uses. \n",
    "\n",
    "In this tutorial, you will examine the different activation functions common in Deep Learning, and gain some intuition for how and when to use each.  \n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center><h3>Part 1: What is a Neuron Computing?</h3></center>\n",
    "\n",
    "Recall the architecture of a typical neural network.  The network is is made out of layers of neurons.  Each neuron in layer **_L_** takes in all the outputs from the previous layer (**_L - 1_**), computes an ouput, and passes that along to every neuron in the next layer (**_L + 1_**). Recall that when we talk about the layers of a neural network, we typically aren't talking about the _input layer_, only the **_hidden layer(s)_** and **_output layer_**.  Take a look at the following diagram for a refresher.  Each circle represents a neuron, and each line represents a weight.  \n",
    "\n",
    "\n",
    "<img src='http://www.astroml.org/_images/fig_neural_network_1.png' height=50% width=50%>\n",
    "\n",
    "But what happens _inside_ each neuron?  Each neuron computes their **_Z_** value, and feeds it through the an **_Activation Function._**  Let's take a look at at a diagram and see how we can gain some intuition for this process.\n",
    "<br>\n",
    "<br>\n",
    "<img src='http://3.bp.blogspot.com/-7RWgohC4pYE/VhtQ8IELsLI/AAAAAAAAA6I/_XFhMbjpcCY/s1600/Simple%2BNeural%2BNetwork.png' height=40% width=40%>\n",
    "\n",
    "Don't let all the mathematical notation in this diagram scare you, it's actually pretty simple! Let's work through it piece by piece before we compute some examples by hand.  \n",
    "\n",
    "\n",
    "\n",
    "Another way to represent the equation shown in the large neuron is **Z = w<sup>T</sup>x + b**.  **Z** is then fed through an **_Activation Function_**, to compute the output that will be propagated to the next layer.  \n",
    "\n",
    "Let's define each variable in this equation to make it easier to understand:\n",
    "\n",
    "**_w:_** A vector containing every weight value from every neuron from the previous layer. These weights start off as random values, but will shift towards optimal values during the training phase through backpropagation.  \n",
    "<br>\n",
    "**_<sup>T</sup>:_** Mathematical notation for \"_Transpose_\", which means \"flip the matrix over its diagonal\". In this case, it takes our 1-dimensional weight vector and rotates it so that we can easily compute the dot product of our weight vector and X, our inputs.  For example, a 4 x 1 vector would become a 1 x 4 vector.  \n",
    "<br>\n",
    "**_x:_** A vector containing all the outputs from the previous layer. The positions of each value in this vector match up with the position of the corresponding weight in the weight vector.  For instance, _w<sub>1</sub>_ and _X<sub>1</sub>_ are the values for the weight and output value from the 1st neuron of the previous layer.  \n",
    "\n",
    "**_b:_** A bias added to the equation to shift the decision boundary away from the origin. Recall that this value is set to a random value at initialization, and each neuron will learn the best bias along with the weights through backpropagation during the training phase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<center><h3>Test Your Understanding</h3></center>\n",
    "\n",
    "Try testing your understanding by manually computing z for the following neurons.  \n",
    "\n",
    "**_Hint:_** No need to do this manually--we need to compute the sum of inputs multiplied by their corresponding weights, plus the bias value.  This is the same as computing the **_dot product_** of w and x, and then adding b.  To compute the dot product of two vectors, just use numpy's [numpy.dot function](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.dot.html). An example is provided as a comment below.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z1:  6.819999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Problem 1: \n",
    "w1 = [-0.4, 0.82, 1.0, 0.5, 0.23, -0.75]\n",
    "x1 = [2, 3, 3, 7, 1, 4]\n",
    "b1 = 1.43\n",
    "\n",
    "#(SAMPLE SOLUTION: PROBLEM 1)\n",
    "z1 = np.dot(w1, x1)\n",
    "z1 += b1\n",
    "print(\"z1:  {}\".format(z1))\n",
    "\n",
    "#Problem 2:\n",
    "w2 = [-0.43, -2.1, 0.3, 0.25, -1.1, -0.43]\n",
    "x2 = [0.22, 0.34, 1.2, 0.00038, .63, -0.22]\n",
    "b2 = -3.4\n",
    "\n",
    "z2 = \n",
    "print(\"z2:  {}\".format(z2))\n",
    "\n",
    "#Problem 3:\n",
    "w3 = [-0.00014, -.31105, 0.3, 0.000256, -.1145, -0.0000009]\n",
    "x3 = [0.22, 0.34, 1.2, 0.00038, .63, -0.22]\n",
    "b3 = 0.0016\n",
    "\n",
    "z3 = \n",
    "print(\"z3:  {}\".format(z3))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected Output\n",
    "z1: 6.819999999999999\n",
    "z2:-4.446905\n",
    "z3 0.18367749528"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Passing <em>z</em> Through Activation Functions</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the neuron"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:MS-ML-P3]",
   "language": "python",
   "name": "conda-env-MS-ML-P3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
