{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Make sure you run this cell!\n",
    "import keras\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Understanding Activation Functions</h1></center>\n",
    "\n",
    "**Goal:** Understand the different activation functions and their common uses. \n",
    "\n",
    "In this tutorial, you will examine the different activation functions common in Deep Learning, and gain some intuition for how and when to use each.  \n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center><h3>Part 1: What is a Neuron Computing?</h3></center>\n",
    "\n",
    "Recall the architecture of a typical neural network.  The network is is made out of layers of neurons.  Each neuron in layer **_L_** takes in all the outputs from the previous layer (**_L - 1_**), computes an ouput, and passes that along to every neuron in the next layer (**_L + 1_**). Recall that when we talk about the layers of a neural network, we typically aren't talking about the _input layer_, only the **_hidden layer(s)_** and **_output layer_**.  Take a look at the following diagram for a refresher.  Each circle represents a neuron, and each line represents a weight.  \n",
    "\n",
    "\n",
    "<img src='http://www.astroml.org/_images/fig_neural_network_1.png' height=50% width=50%>\n",
    "\n",
    "But what happens _inside_ each neuron?  Each neuron computes their **_Z_** value, and feeds it through the an **_Activation Function._**  Let's take a look at at a diagram and see how we can gain some intuition for this process.\n",
    "<br>\n",
    "<br>\n",
    "<img src='http://3.bp.blogspot.com/-7RWgohC4pYE/VhtQ8IELsLI/AAAAAAAAA6I/_XFhMbjpcCY/s1600/Simple%2BNeural%2BNetwork.png' height=40% width=40%>\n",
    "\n",
    "Don't let all the mathematical notation in this diagram scare you, it's actually pretty simple! Let's work through it piece by piece before we compute some examples by hand.  \n",
    "\n",
    "\n",
    "\n",
    "Another way to represent the equation shown in the large neuron is **Z = w<sup>T</sup>x + b**.  **Z** is then fed through an **_Activation Function_**, to compute the output that will be propagated to the next layer.  \n",
    "\n",
    "Let's define each variable in this equation to make it easier to understand:\n",
    "\n",
    "**_w:_** A vector containing every weight value from every neuron from the previous layer. These weights start off as random values, but will shift towards optimal values during the training phase through backpropagation.  \n",
    "<br>\n",
    "**_<sup>T</sup>:_** Mathematical notation for \"_Transpose_\", which means \"flip the matrix over its diagonal\". In this case, it takes our 1-dimensional weight vector and rotates it so that we can easily compute the dot product of our weight vector and X, our inputs.  For example, a 4 x 1 vector would become a 1 x 4 vector.  \n",
    "<br>\n",
    "**_x:_** A vector containing all the outputs from the previous layer. The positions of each value in this vector match up with the position of the corresponding weight in the weight vector.  For instance, _w<sub>1</sub>_ and _X<sub>1</sub>_ are the values for the weight and output value from the 1st neuron of the previous layer.  \n",
    "\n",
    "**_b:_** A bias added to the equation to shift the decision boundary away from the origin. Recall that this value is set to a random value at initialization, and each neuron will learn the best bias along with the weights through backpropagation during the training phase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<center><h3>Test Your Understanding</h3></center>\n",
    "\n",
    "Try testing your understanding by manually computing z for the following neurons.  \n",
    "\n",
    "**_Hint:_** No need to do this manually--we need to compute the sum of inputs multiplied by their corresponding weights, plus the bias value.  This is the same as computing the **_dot product_** of w and x, and then adding b.  To compute the dot product of two vectors, just use numpy's [numpy.dot function](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.dot.html). An example is provided as a comment below.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-39962d9779de>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-39962d9779de>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    z2 =\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Problem 1: \n",
    "w1 = [-0.4, 0.82, 1.0, 0.5, 0.23, -0.75]\n",
    "x1 = [2, 3, 3, 7, 1, 4]\n",
    "b1 = 1.43\n",
    "\n",
    "#(SAMPLE SOLUTION: PROBLEM 1)\n",
    "z1 = np.dot(w1, x1)\n",
    "z1 += b1\n",
    "print(\"z1:  {}\".format(z1))\n",
    "\n",
    "#Problem 2:\n",
    "w2 = [-0.43, -2.1, 0.3, 0.25, -1.1, -0.43]\n",
    "x2 = [0.22, 0.34, 1.2, 0.00038, .63, -0.22]\n",
    "b2 = -3.4\n",
    "\n",
    "z2 = \n",
    "print(\"z2:  {}\".format(z2))\n",
    "\n",
    "#Problem 3:\n",
    "w3 = [-0.00014, -.31105, 0.3, 0.000256, -.1145, -0.0000009]\n",
    "x3 = [0.22, 0.34, 1.2, 0.00038, .63, -0.22]\n",
    "b3 = 0.0016\n",
    "\n",
    "z3 = \n",
    "print(\"z3:  {}\".format(z3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "<br>\n",
    "z1: 6.819999999999999\n",
    "<br>\n",
    "z2:-4.446905\n",
    "<br>\n",
    "z3 0.18367749528"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2> What is an Activation Function?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neuron has now taken all weights and inputs, reduced them down to a single value, and added our bias.  What do we do with this number now?\n",
    "\n",
    "We feed through an **_Activation Function_**, which will determine what message the neuron passes on to the next layer.  In order to understand this, it's helpful to remember the inspiration behind neural networks--neurons in a brain!\n",
    "\n",
    "<img src='https://i.giphy.com/media/FLw63FKxHyVHO/giphy.webp'>\n",
    "\n",
    "A neural network is a _symbolic representation_ of the structure and function of neurons in a brain.  This means that although the details may be different, the general purpose and structure are the same--neural networks are a symbolic representation of neurons in a brain in the same way that airplanes are a symbolic representation of birds.\n",
    "\n",
    "In a brain, a neuron takes in the inputs from other neurons (through the dendrites on the left side of the picture) , and if the combination of those inputs are enough, it \"fires\" by passing an electrical charge down through the axon to any other connected neurons.  A neuron either fires or doesnt fire--on or off.  There is no \"in between\" state.  In this way, the neurons of our neural network are different--depending on the **_activation function_** used, our neurons do not have this limitation.  \n",
    "\n",
    "In simpler versions such as Single-Layer Perceptrons, the activation function is a simple step function. If the value of Z is greater than 1, the perceptron fires, passing a value of 1.  Otherwise, it does not fire, passing along a value of 0.  This may work for perceptrons, but for Deep Neural Networks, it leaves a lot to be desired. We'll skip over the step activation function and move right onto the activation functions most commonly used in Deep Learning: **_Sigmoid, Tanh, reLU,_** and **_Softmax_**\n",
    "\n",
    "From here on out, we'll code up examples of every activation function from scratch to get an intuition for how each one works.  We'll also explore the best use cases for each by looking at an examples of each in real neural networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing our training and testing sets, don't worry if you don't \n",
    "# Understand this part--we'll cover this in depth in other tutorials!\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(60000, 784).astype('float32')\n",
    "X_test = X_test.reshape(10000, 784).astype('float32')\n",
    "X_train /= 255.\n",
    "X_test /= 255.\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Sigmoid</h2></center>\n",
    "\n",
    "The **_sigmoid_** maps all possible real number inputs from negative infinity to infinity to a value between 0 and 1.  The function is called a sigmoid because when graphed, it resembles an 'S'.  If you're familiar with logistic regression, you'll likely recognize this function.    \n",
    "\n",
    "<center><img src='https://ipfs.io/ipfs/QmXoypizjW3WknFiJnKLwHCnL72vedxjQkDDP1mXWo6uco/I/m/Logistic-curve.svg.png'></center>\n",
    "\n",
    "The equation for the sigmoid function is: \n",
    "\n",
    "<center><img src='http://file.scirp.org/Html/htmlimages/10-9402081x/34ca7298-136f-4b25-a192-1e7f24401b53.png'></center>\n",
    "\n",
    "In this case, **x** will be equal to Z, which is equal to the dot product of our weights and inputs, plus the bias value. \n",
    "\n",
    "**_Challenge:_** Complete the function below so that it returns the logistic sigmoid of the input.  (Hint: make use of numpy's [numpy.exp() function!](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.exp.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    s = None\n",
    "    return s\n",
    "\n",
    "print('test 1: {}    Expected output: {}'.format(sigmoid(1.5),  0.817574476))\n",
    "print('test 2: {}    Expected output: {}'.format(sigmoid(-0.2), 0.450166002))\n",
    "print('test 3: {}    Expected output: {}'.format(sigmoid(6.7),  0.998770601))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Use Cases: Sigmoid</h3></center>\n",
    "\n",
    "A sigmoid function can be used as a common activation function for any typical layer in a neural network.  This is a simple but effective way to introduce non-linearity into our model.  Although there are other activation functions that have become more popular in Deep Learning as of late, it is still very common to see sigmoids as the activation function for the output layer in binary classifiers.  This makes intuitive sense, since the _output of a sigmoid can be easily interpreted as a percentage value between 0 and 100%_!\n",
    "\n",
    "<center><h3>Example Use: Sigmoid</h3></center>\n",
    "\n",
    "In this example, we'll build an image classifier using the popular MNIST data set.  During this step, we'll use sigmoid activation functions in every layer except for the last one, which will be a softmax (there are 10 possible classes, so a sigmoid activation function would not make sense here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 2.2926 - acc: 0.1363 - val_loss: 2.2588 - val_acc: 0.1466\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 2.2369 - acc: 0.2943 - val_loss: 2.2095 - val_acc: 0.2861\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 2.1806 - acc: 0.4012 - val_loss: 2.1418 - val_acc: 0.4861\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 2.1002 - acc: 0.4889 - val_loss: 2.0437 - val_acc: 0.4808\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 1.9850 - acc: 0.5169 - val_loss: 1.9068 - val_acc: 0.5363\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 1.8355 - acc: 0.5549 - val_loss: 1.7434 - val_acc: 0.5627\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 1.6706 - acc: 0.5890 - val_loss: 1.5785 - val_acc: 0.5950\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 1.5145 - acc: 0.6246 - val_loss: 1.4307 - val_acc: 0.6390\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 1.3768 - acc: 0.6591 - val_loss: 1.3013 - val_acc: 0.6758\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 1.2571 - acc: 0.6853 - val_loss: 1.1895 - val_acc: 0.7040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2089ef2fb70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense((64), activation='sigmoid', input_shape=(784,)))\n",
    "model.add(Dense((64), activation='sigmoid', input_shape=(784,)))\n",
    "model.add(Dense((10), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Takeaways: Sigmoid Activation Functions</h3></center>\n",
    "\n",
    "After 10 epochs of training on the MNIST data set, we were able to obtain an **accuracy of 70%** on our validation set--not bad!  There's still room for improvement here.  This shows us that sigmoid activation functions do a passable job at capturing nonlinearity presented in the model.  Let's hold on to that 70% number and see how it stacks up against other activation functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Tanh</h2></center>\n",
    "\n",
    "Tanh (pronounched \"tanch\") stands for  _Hyperbolic Tangent_.  When plotted, the tanh function looks very similar to the sigmoid activation function.  This is because tanh is just a rescaled logistic sigmoid function! The main difference between the two is whereas the sigmoid activation function is bounded at 0 and 1, the tanh activation function is bounded at -1 and 1.  Let's take a look at a graph of the tanh function:\n",
    "\n",
    "<img src='http://mathworld.wolfram.com/images/interactive/TanhReal.gif'>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Take a look at the plot below to see how tanh compares to the sigmoid function. The red line is a plot of the <font color='red'>tanh function</font>, and the blue line is the <font color='blue'>sigmoid function</font>. \n",
    "\n",
    "<img src='http://brenocon.com/blog/wp-content/uploads/2013/10/Screen-Shot-2013-10-31-at-4.32.04-PM.png' height=50% width=50%>\n",
    "\n",
    "\n",
    "<center><h3>Use Cases: Tanh</h3></center>\n",
    "The tanh function is generally superior to the sigmoid function for training neural networks.  There are two main reasons for this:\n",
    "\n",
    "1.  Stronger gradients, which allow for more efficient back propagation.  \n",
    "2.  (Normalized) Data will be centered around 0 with a tanh activation function.  This is important, since the output of this layer will be the input of the next layer.  This helps prevent internal covariate shift--where each successive layer starts slightly biasing in a continuing direction (for intuition on this, take a look at what 0 outputs on a sigmoid function, and then use that as your input.  How did it change?)\n",
    "\n",
    "If you're interested in really digging into the reasons why tanh is a better choice than a sigmoid function, check out Yann LeCunn's excellent paper on <a href='http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf'>Efficient Backprop</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 1.1338 - acc: 0.7361 - val_loss: 0.6580 - val_acc: 0.8565\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.5611 - acc: 0.8662 - val_loss: 0.4639 - val_acc: 0.8869\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.4404 - acc: 0.8860 - val_loss: 0.3921 - val_acc: 0.8974\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.3857 - acc: 0.8959 - val_loss: 0.3520 - val_acc: 0.9043\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.3532 - acc: 0.9027 - val_loss: 0.3267 - val_acc: 0.9095\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.3307 - acc: 0.9071 - val_loss: 0.3103 - val_acc: 0.9144\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.3136 - acc: 0.9119 - val_loss: 0.2957 - val_acc: 0.9189\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.2998 - acc: 0.9151 - val_loss: 0.2835 - val_acc: 0.9210\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.2881 - acc: 0.9186 - val_loss: 0.2745 - val_acc: 0.9244\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.2778 - acc: 0.9211 - val_loss: 0.2663 - val_acc: 0.9248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2089f2c7f98>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh_model = Sequential()\n",
    "tanh_model.add(Dense((64), activation='tanh', input_shape=(784,)))\n",
    "tanh_model.add(Dense((64), activation='tanh'))\n",
    "tanh_model.add(Dense((10), activation='softmax'))\n",
    "tanh_model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])\n",
    "tanh_model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>reLU/Leaky reLU</h2></center>\n",
    "\n",
    "TODO: Add text section about reLu activation function.\n",
    "\n",
    "TODO: Add graph of reLu activation function.\n",
    "\n",
    "TODO: Compare to leaky reLu and mention vanishing/exploding gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 1.3871 - acc: 0.6429 - val_loss: 0.6622 - val_acc: 0.8433\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.5363 - acc: 0.8614 - val_loss: 0.4260 - val_acc: 0.8848\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.4071 - acc: 0.8886 - val_loss: 0.3577 - val_acc: 0.9006\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.3561 - acc: 0.9000 - val_loss: 0.3207 - val_acc: 0.9097\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.3257 - acc: 0.9074 - val_loss: 0.2990 - val_acc: 0.9163\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.3041 - acc: 0.9137 - val_loss: 0.2826 - val_acc: 0.9204\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.2876 - acc: 0.9186 - val_loss: 0.2691 - val_acc: 0.9243\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.2740 - acc: 0.9219 - val_loss: 0.2589 - val_acc: 0.9261\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.2625 - acc: 0.9253 - val_loss: 0.2482 - val_acc: 0.9286\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.2523 - acc: 0.9280 - val_loss: 0.2393 - val_acc: 0.9302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2091758de48>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_model = Sequential()\n",
    "relu_model.add(Dense((64), activation='relu', input_shape=(784,)))\n",
    "relu_model.add(Dense((64), activation='relu'))\n",
    "relu_model.add(Dense((10), activation='softmax'))\n",
    "relu_model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])\n",
    "relu_model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Softmax</h2></center>\n",
    "\n",
    "TODO: Add text section about Softmax activation function\n",
    "\n",
    "TODO: Find good visualization for softmax.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2091b715f98>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2089cb987b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "(X_pred, _), _= mnist.load_data()\n",
    "reshape_pred = X_pred.reshape(60000, 784).astype(\"float32\")\n",
    "reshape_pred /= 255.\n",
    "plt.imshow(X_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of Softmax Layer: [[  1.85263413e-03   2.50765606e-05   2.71464814e-03 ...,   4.03382932e-04\n",
      "    9.36570927e-04   4.18095209e-04]\n",
      " [  9.99591887e-01   1.38157390e-07   7.74314503e-06 ...,   7.99087502e-06\n",
      "    3.98092925e-06   2.12829818e-05]\n",
      " [  7.85107666e-04   3.04626371e-03   8.98579322e-03 ...,   7.05117919e-03\n",
      "    7.82220997e-03   7.51359463e-02]\n",
      " ..., \n",
      " [  1.73432069e-04   1.39954100e-05   1.81558778e-06 ...,   8.18724220e-05\n",
      "    4.06661537e-03   6.96342811e-03]\n",
      " [  1.40525009e-02   3.91526846e-05   2.71060644e-03 ...,   4.18891548e-04\n",
      "    1.10156114e-04   9.22629319e-04]\n",
      " [  6.08780570e-02   3.26693669e-04   2.09652688e-02 ...,   1.14168385e-02\n",
      "    7.33409107e-01   1.39977574e-01]]\n",
      "Prediction: 5\n",
      "Certainty: 85.55%\n"
     ]
    }
   ],
   "source": [
    "preds = relu_model.predict(reshape_pred)\n",
    "print(\"Output of Softmax Layer: {}\".format(preds))\n",
    "print(\"Prediction: {}\".format(np.argmax(preds[0])))\n",
    "print(\"Certainty: {:.2%}\".format(np.max(preds[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
